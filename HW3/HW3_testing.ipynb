{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data\n",
    "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
    "See follow links for more information\n",
    "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "\n",
    "# drop some unwanted rows\n",
    "train_df = train_df.drop([248, 250, 251]) # cp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>fixed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "136   54    1   2       192   283    0        2      195      0      0.0   \n",
       "232   58    0   4       170   225    1        2      146      1      2.8   \n",
       "233   56    1   2       130   221    0        2      163      0      0.0   \n",
       "184   46    1   4       120   249    0        2      144      0      0.8   \n",
       "84    55    0   2       135   250    0        2      161      0      1.4   \n",
       "\n",
       "     slope  ca        thal  target  \n",
       "136      1   1  reversible       0  \n",
       "232      2   2       fixed       1  \n",
       "233      1   0  reversible       0  \n",
       "184      1   0  reversible       0  \n",
       "84       2   0      normal       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df.info()\n",
    "# test_df.info()\n",
    "train_df.columns\n",
    "# test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding\n",
    "- features: 'cp', 'restecg', 'thal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding_list = ['cp', 'restecg', 'thal', 'slope', 'ca']\n",
    "encoding_list = ['cp', 'restecg', 'thal']\n",
    "for feature in encoding_list:\n",
    "    train_df = pd.get_dummies(train_df, columns=[feature], prefix = [feature])\n",
    "    test_df = pd.get_dummies(test_df, columns=[feature], prefix = [feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
      "       'slope', 'ca', 'target', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0',\n",
      "       'restecg_1', 'restecg_2', 'thal_fixed', 'thal_normal',\n",
      "       'thal_reversible'],\n",
      "      dtype='object')\n",
      "Index(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
      "       'slope', 'ca', 'target', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0',\n",
      "       'restecg_1', 'restecg_2', 'thal_fixed', 'thal_normal',\n",
      "       'thal_reversible'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print(train_df.head())\n",
    "# print(test_df.head())\n",
    "print(train_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(train_df.columns)\n",
    "features.remove('target')\n",
    "X_train_df, y_train_df = train_df[features], train_df['target']\n",
    "X_test_df, y_test_df = test_df[features], test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_df.to_numpy(), y_train_df.to_numpy()\n",
    "X_test, y_test = X_test_df.to_numpy(), y_test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 20) (198,)\n",
      "(100, 20) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    if np.unique(sequence).shape[0] == 1:  # pure\n",
    "        return 0\n",
    "\n",
    "    hist = np.bincount(sequence) / len(sequence)\n",
    "    return 1 - np.sum(hist ** 2)\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    if np.unique(sequence).shape[0] == 1:\n",
    "        return 0\n",
    "\n",
    "    hist = np.bincount(sequence) / len(sequence)\n",
    "    hist = hist[hist != 0]  # eliminate zeros\n",
    "    return - np.sum(hist * log2(hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, X: np.ndarray,\n",
    "                 Y: np.ndarray,\n",
    "                 depth, criterion='gini',\n",
    "                 max_depth=None,\n",
    "                 sample_weight=None,\n",
    "                 max_features=None):\n",
    "        \"\"\"\n",
    "            X(np arr): features\n",
    "            Y(np arr): labels\n",
    "        \"\"\"\n",
    "        self.right, self.left = None, None\n",
    "        self.X, self.Y = X, Y\n",
    "        self.depth = depth\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.best_feature, self.best_split_value = None, None\n",
    "        self.cls = None\n",
    "        self.is_leaf = False\n",
    "        self.sample_weight = sample_weight\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def gini(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        hist = hist[hist != 0]  # eliminate zeros\n",
    "        return - np.sum(hist * log2(hist))\n",
    "\n",
    "    def weighted_gini(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (gini) \"\"\"\n",
    "        return (len(Y_r) * self.gini(Y_r) + len(Y_l) * self.gini(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def weighted_entropy(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (entropy) \"\"\"\n",
    "        return (len(Y_r) * self.entropy(Y_r) + len(Y_l) * self.entropy(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def sample_weight_gini(self, Y: np.ndarray, sample_weight: np.ndarray):\n",
    "        cls_list = np.unique(Y)\n",
    "\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if len(cls_list) == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        cls_count = np.zeros(len(cls_list))\n",
    "\n",
    "        for cls in cls_list:\n",
    "            cls_count[cls] = sample_weight[Y == cls].sum()\n",
    "\n",
    "        hist = cls_count / sample_weight.sum()\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def sw_gini(self, Y_r, Y_l, sw_r, sw_l):\n",
    "        \"\"\"  sample weighted  \"\"\"\n",
    "        gini = lambda Y, sample_weight: self.sample_weight_gini(Y, sample_weight)\n",
    "        len_r, len_l = sw_r.sum(), sw_l.sum()\n",
    "        len_tot = len_r + len_l\n",
    "        return (len_r * gini(Y_r, sw_r) + len_l * gini(Y_l, sw_l)) / len_tot\n",
    "\n",
    "    def sw_best_split(self):\n",
    "        \"\"\" (w/ sample weight) find the best split \"\"\"\n",
    "        X, Y, sample_weight = self.X.copy(), self.Y.copy(), self.sample_weight.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        for feature in range(len(X[0])):\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                mask = (X[:, feature] == 0)\n",
    "                inv_mask = (mask == False)\n",
    "                Y_l, sw_l = Y[mask], sample_weight[mask]\n",
    "                Y_r, sw_r = Y[inv_mask], sample_weight[inv_mask]\n",
    "\n",
    "                Gini = self.sw_gini(Y_r, Y_l, sw_r, sw_l)\n",
    "                # print('feature:', feature,  'gini:',Gini)  #FIXME\n",
    "                if Gini < min_gini:\n",
    "                    min_gini = Gini\n",
    "                    best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y, sample_weight = X[idx], Y[idx], sample_weight[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    mask = (X[:, feature] <= threshold)\n",
    "                    inv_mask = (mask == False)\n",
    "                    Y_l, sw_l = Y[mask], sample_weight[mask]\n",
    "                    Y_r, sw_r = Y[inv_mask], sample_weight[inv_mask]\n",
    "\n",
    "                    Gini = self.sw_gini(Y_r, Y_l, sw_r, sw_l)\n",
    "                    # print('feature:', feature,  'gini:',Gini)  #FIXME\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def sw_split(self):\n",
    "        \"\"\" (w/ sample weight) split this node recursively \"\"\"\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.sw_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l, sw_l = self.X[mask_l, :], self.Y[mask_l], self.sample_weight[mask_l]\n",
    "        X_r, Y_r, sw_r = self.X[mask_r, :], self.Y[mask_r], self.sample_weight[mask_r]\n",
    "\n",
    "        lnode = Node(X_l, Y_l, self.depth + 1, self.criterion, self.max_depth, sw_l)\n",
    "        rnode = Node(X_r, Y_r, self.depth + 1, self.criterion, self.max_depth, sw_r)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "\n",
    "        rnode.sw_split()\n",
    "        lnode.sw_split()\n",
    "\n",
    "    def rand_best_split(self):\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        # randomly sample features\n",
    "        feature_idx = np.random.choice(len(X[0]), size=self.max_features, replace=False)\n",
    "\n",
    "        for feature in feature_idx:\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def best_split(self):\n",
    "        \"\"\" find the best split \"\"\"\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        for feature in range(len(X[0])):\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" split this node recursively \"\"\"\n",
    "\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "        # FIXME\n",
    "        # print('depth:', self.depth)\n",
    "        # print('cls 0:', cls_count[0])\n",
    "        # print('cls 1:', cls_count[1])\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "\n",
    "            # FIXME\n",
    "            # print('predicted class:', self.cls)\n",
    "            # print()\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.best_split() if self.max_features is None \\\n",
    "            else self.rand_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # FIXME\n",
    "        # print('best_feature: ', best_feature, 'best_split_value: ', best_split_value)\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l = self.X[mask_l, :], self.Y[mask_l]\n",
    "        X_r, Y_r = self.X[mask_r, :], self.Y[mask_r]\n",
    "\n",
    "        lnode = Node(X_l, Y_l, self.depth + 1, self.criterion, self.max_depth)\n",
    "        rnode = Node(X_r, Y_r, self.depth + 1, self.criterion, self.max_depth)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "        # print()\n",
    "\n",
    "        rnode.split()\n",
    "        lnode.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        if criterion not in ['gini', 'entropy']:\n",
    "            raise Exception(\"supported criterion: 'gini' or 'entropy'\")\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, sample_weight=None, max_features=None):\n",
    "        \"\"\" Build a decision tree classifier from the training set (X, y).\"\"\"\n",
    "        self.X, self.Y = X.copy(), Y.copy()\n",
    "        self.root = Node(self.X,\n",
    "                         self.Y,\n",
    "                         depth=0,\n",
    "                         criterion=self.criterion,\n",
    "                         max_depth=self.max_depth,\n",
    "                         sample_weight=sample_weight,\n",
    "                         max_features=max_features)\n",
    "\n",
    "        if sample_weight is None:\n",
    "            self.root.split()\n",
    "        else:\n",
    "            self.root.sw_split()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict class value for X. \"\"\"\n",
    "        y_pred = []\n",
    "\n",
    "        for row in X_test:\n",
    "            cur_node = self.root\n",
    "\n",
    "            while not cur_node.is_leaf:\n",
    "                bf = cur_node.best_feature\n",
    "                cur_node = cur_node.left if row[bf] <= cur_node.best_split_value else cur_node.right\n",
    "\n",
    "            y_pred.append(cur_node.cls)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def feature_importance(self) -> np.ndarray:\n",
    "        \"\"\" count occurrence of each best_feature \"\"\"\n",
    "        cur_node = self.root\n",
    "\n",
    "        self.feature_count = np.zeros(len(self.X[0]), dtype='uint8')\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "        return self.feature_count\n",
    "\n",
    "    def count_feature_util(self, cur_node: Node):\n",
    "        if cur_node.is_leaf:\n",
    "            return\n",
    "\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.fit(X_train,y_train)\n",
    "y_pred = clf_depth3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 3\n",
      "Test-set accuarcy score:  0.79\n"
     ]
    }
   ],
   "source": [
    "print('Depth = 3')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "clf_depth10.fit(X_train,y_train)\n",
    "y_pred = clf_depth10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 10\n",
      "Test-set accuarcy score:  0.76\n"
     ]
    }
   ],
   "source": [
    "print('Depth = 10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.fit(X_train,y_train)\n",
    "y_pred = clf_gini.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini\n",
      "Test-set accuarcy score:  0.79\n"
     ]
    }
   ],
   "source": [
    "print('Gini')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.fit(X_train,y_train)\n",
    "y_pred = clf_entropy.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entopry\n",
      "Test-set accuarcy score:  0.79\n"
     ]
    }
   ],
   "source": [
    "print('Entopry')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "       'exang', 'oldpeak', 'slope', 'ca', 'thal'])\n",
    "\n",
    "# ['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
    "#        'slope', 'ca', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0',\n",
    "#        'restecg_1', 'restecg_2', 'thal_fixed', 'thal_normal',\n",
    "#        'thal_reversible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = clf_depth10.feature_importance()\n",
    "hist = np.zeros(13, dtype='uint8')\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0:10] = tmp[0:10]\n",
    "hist[-3:] = [tmp[10:10+4].sum(),tmp[14:14+3].sum(),tmp[17:17+3].sum()]\n",
    "print(hist)\n",
    "mask_sort = hist.argsort()\n",
    "print(mask_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title('Feature Importance', {'fontsize':20})\n",
    "ax.barh(features[mask_sort], hist[mask_sort])\n",
    "\n",
    "for i in range(10):\n",
    "    ax.axvline(x=i, linewidth=1, color='0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.weak_clf = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        self.alphas = []\n",
    "        n = self.n_estimators\n",
    "        max_depth = 1\n",
    "\n",
    "        self.Y = Y.copy()\n",
    "        # change label: 0 -> -1\n",
    "        self.Y[Y == 0] = -1\n",
    "\n",
    "        data_w = np.ones(len(Y)) / len(Y)\n",
    "\n",
    "        # train n weak classifiers\n",
    "        for i in range(n):\n",
    "            # FIXME:\n",
    "            # print(i, \"th weak clf\")\n",
    "\n",
    "            # Fit weak classifier\n",
    "            clf = DecisionTree(criterion='gini', max_depth=max_depth)\n",
    "            clf.fit(X, Y, sample_weight=data_w)\n",
    "            y_pred = clf.predict(X)\n",
    "            y_pred[y_pred == 0] = -1\n",
    "            print(clf.root.best_feature)\n",
    "\n",
    "            # error of the clf prediction\n",
    "            error = data_w[self.Y != y_pred].sum()\n",
    "\n",
    "            # handle boundaries\n",
    "            if error >= 1.0:\n",
    "                break\n",
    "            elif error <= 0.0:\n",
    "                alpha = 200.0\n",
    "            else:\n",
    "                alpha = np.log((1 - error) / error)\n",
    "\n",
    "            self.weak_clf.append(clf)\n",
    "            # alpha: weight of classifier\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            #  Update data weights\n",
    "            data_w = data_w * np.exp(- self.alphas[i] * self.Y * y_pred)\n",
    "            data_w = data_w / data_w.sum()\n",
    "\n",
    "            # FIXME\n",
    "            # print(\"error: \", error)\n",
    "            # print(\"alpha: \", alpha)\n",
    "            # print()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        prediction = np.zeros(len(X_test))\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred = self.weak_clf[i].predict(X_test)\n",
    "            y_pred[y_pred == 0] = -1\n",
    "            prediction += self.alphas[i] * y_pred\n",
    "\n",
    "        prediction = np.sign(prediction)\n",
    "        prediction[prediction < 0] = 0\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_Ada_10 = AdaBoost(n_estimators=10)\n",
    "clf_Ada_10.fit(X_train,y_train)\n",
    "y_pred = clf_Ada_10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('n_estimators=10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_Ada_100 = AdaBoost(n_estimators=100)\n",
    "clf_Ada_100.fit(X_train,y_train)\n",
    "y_pred = clf_Ada_100.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('n_estimators=100')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self,\n",
    "                 n_estimators,\n",
    "                 max_features,\n",
    "                 boostrap=True,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.weak_clf = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        for i in range(self.n_estimators):\n",
    "            # bootstrapping\n",
    "            if self.boostrap:\n",
    "                sample_idx = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            else:\n",
    "                sample_idx = np.arange(len(X))\n",
    "\n",
    "            clf = DecisionTree(criterion=self.criterion, max_depth=self.max_depth)\n",
    "            clf.fit(X[sample_idx], Y[sample_idx], max_features=self.max_features)\n",
    "            self.weak_clf.append(clf)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.zeros(len(X_test), dtype='int64')\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred += self.weak_clf[i].predict(X_test)\n",
    "\n",
    "        mask = (y_pred > (self.n_estimators / 2))\n",
    "        y_pred[mask] = 1\n",
    "        y_pred[(mask == False)] = 0\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]).astype('uint32'), max_depth=None)\n",
    "clf_10tree.fit(X_train,y_train)\n",
    "y_pred = clf_10tree.predict(X_test)\n",
    "\n",
    "print('n_estimators=10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(X_train.shape[1]).astype('uint32'), max_depth=None)\n",
    "clf_100tree.fit(X_train,y_train)\n",
    "y_pred = clf_100tree.predict(X_test)\n",
    "\n",
    "print('n_estimators=100')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]).astype('uint32'), max_depth=None)\n",
    "clf_random_features.fit(X_train,y_train)\n",
    "y_pred = clf_random_features.predict(X_test)\n",
    "\n",
    "print('max_features=sqrt(n_features)')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_all_features = RandomForest(n_estimators=10, max_features=X_train.shape[1], max_depth=None)\n",
    "clf_all_features.fit(X_train,y_train)\n",
    "y_pred = clf_all_features.predict(X_test)\n",
    "\n",
    "print('max_features=n_features')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 6.\n",
    "Try you best to get highest test accuracy score by \n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = your_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,100):\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=n, learning_rate=0.05, max_depth=1, random_state=0)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "    score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "    print(f'Train score={train_score}, Test score={score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in range(1,21):\n",
    "    clf = DecisionTreeClassifier(criterion='gini', max_depth=depth)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "    score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "    print(f'depth = {depth}, Train score={train_score}, Test score={score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=14)\n",
    "# clf = clf.fit(X_train,y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "max_score = 0.0\n",
    "for n in range (1, 101):\n",
    "    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=n)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "    score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "    max_score = max(max_score, score)\n",
    "    \n",
    "    print(f'n = {n}, Train score={train_score}, Test score={score}')\n",
    "\n",
    "print(max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_score = 0.0\n",
    "max_depth=2\n",
    "random_state=37\n",
    "# optimal: max_depth = 2\n",
    "# for random_state in range(1, 101):\n",
    "clf = RandomForestClassifier(n_estimators=15, max_depth=max_depth, bootstrap=False, random_state=random_state)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "max_score = max(max_score, score)\n",
    "print(f'state = {random_state}, Train score={train_score}, Test score={score}')\n",
    "    \n",
    "print(max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n in range (101):\n",
    "    xgboost_clf = XGBClassifier(n_estimators=n, learning_rate= 0.05, use_label_encoder=False)\n",
    "    xgboost_clf.fit(X_train, y_train)\n",
    "#     y_pred, y_pred_train = xgboost_clf.predict(X_test), xgboost_clf.predict(X_train)\n",
    "\n",
    "#     score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "#     print(f'n = {n}, Train score={train_score}, Test score={score}')\n",
    "    print('訓練集: ',xgboost_clf.score(X_train,y_train))\n",
    "    print('測試集: ',xgboost_clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# My model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for depth in range(1, 21):\n",
    "depth = 2\n",
    "clf = DecisionTree(criterion='gini', max_depth=depth)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_train, y_pred = clf.predict(X_train), clf.predict(X_test)\n",
    "score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f'max_depth = {depth}, Train score={train_score}, Test score={score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoost(n_estimators=59)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Train-set accuarcy score: ', accuracy_score(y_train, y_pred_train))\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# optimal: max_depth=1, n_estimators=2, test_score=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_score = 0.0\n",
    "for n in range (1, 101):\n",
    "    clf = AdaBoost(n_estimators=n)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "    score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "    max_score = max(max_score, score)\n",
    "        \n",
    "    print(f'n_estimators = {n} Train score={train_score}, Test score={score}')\n",
    "\n",
    "# depth = 3, n_estimators = 10, Test score=0.81\n",
    "# depth = 1, n_estimators = 2, Test score=0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node_Pro():\n",
    "    def __init__(self, \n",
    "                 X: np.ndarray,\n",
    "                 Y: np.ndarray,\n",
    "                 depth, \n",
    "                 criterion='gini',\n",
    "                 max_depth=None,\n",
    "                 max_features=None, \n",
    "                 init_seed=None):\n",
    "        \"\"\"\n",
    "            X(np arr): features\n",
    "            Y(np arr): labels\n",
    "        \"\"\"\n",
    "        self.right, self.left = None, None\n",
    "        self.X, self.Y = X, Y\n",
    "        self.depth = depth\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.best_feature, self.best_split_value = None, None\n",
    "        self.cls = None\n",
    "        self.is_leaf = False\n",
    "        self.max_features = max_features\n",
    "        self.init_seed = init_seed\n",
    "\n",
    "    def gini(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        hist = hist[hist != 0]  # eliminate zeros\n",
    "        return - np.sum(hist * log2(hist))\n",
    "\n",
    "    def weighted_gini(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (gini) \"\"\"\n",
    "        return (len(Y_r) * self.gini(Y_r) + len(Y_l) * self.gini(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def weighted_entropy(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (entropy) \"\"\"\n",
    "        return (len(Y_r) * self.entropy(Y_r) + len(Y_l) * self.entropy(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def rand_best_split(self):\n",
    "        ''' \n",
    "            sample a subset of features\n",
    "            used by Random forest \n",
    "        '''\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        # randomly sample features\n",
    "        np.random.seed(self.init_seed)\n",
    "        feature_idx = np.random.choice(len(X[0]), size=self.max_features, replace=False)\n",
    "\n",
    "        for feature in feature_idx:\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" split this node recursively \"\"\"\n",
    "\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "        # FIXME\n",
    "        # print('depth:', self.depth)\n",
    "        # print('cls 0:', cls_count[0])\n",
    "        # print('cls 1:', cls_count[1])\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "\n",
    "            # FIXME\n",
    "            # print('predicted class:', self.cls)\n",
    "            # print()\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.rand_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # FIXME\n",
    "        # print('best_feature: ', best_feature, 'best_split_value: ', best_split_value)\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l = self.X[mask_l, :], self.Y[mask_l]\n",
    "        X_r, Y_r = self.X[mask_r, :], self.Y[mask_r]\n",
    "\n",
    "        lnode = Node_Pro(X_l, Y_l, self.depth + 1, self.criterion, self.max_depth, self.max_features, 2*self.init_seed)\n",
    "        rnode = Node_Pro(X_r, Y_r, self.depth + 1, self.criterion, self.max_depth, self.max_features, 2*self.init_seed+1)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "        # print()\n",
    "\n",
    "        rnode.split()\n",
    "        lnode.split()\n",
    "\n",
    "\n",
    "class DecisionTree_Pro():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        if criterion not in ['gini', 'entropy']:\n",
    "            raise Exception(\"supported criterion: 'gini' or 'entropy'\")\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, max_features=None, init_seed=None):\n",
    "        \"\"\" Build a decision tree classifier from the training set (X, y).\"\"\"\n",
    "        self.X, self.Y = X.copy(), Y.copy()\n",
    "        self.root = Node_Pro(self.X,\n",
    "                         self.Y,\n",
    "                         depth=0,\n",
    "                         criterion=self.criterion,\n",
    "                         max_depth=self.max_depth,\n",
    "                         max_features=max_features, init_seed=init_seed)\n",
    "\n",
    "\n",
    "        self.root.split()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict class value for X. \"\"\"\n",
    "        y_pred = []\n",
    "\n",
    "        for row in X_test:\n",
    "            cur_node = self.root\n",
    "\n",
    "            while not cur_node.is_leaf:\n",
    "                bf = cur_node.best_feature\n",
    "                cur_node = cur_node.left if row[bf] <= cur_node.best_split_value else cur_node.right\n",
    "\n",
    "            y_pred.append(cur_node.cls)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def feature_importance(self) -> np.ndarray:\n",
    "        \"\"\" count occurrence of each best_feature \"\"\"\n",
    "        cur_node = self.root\n",
    "\n",
    "        self.feature_count = np.zeros(len(self.X[0]), dtype='uint8')\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "        return self.feature_count\n",
    "\n",
    "    def count_feature_util(self, cur_node):\n",
    "        if cur_node.is_leaf:\n",
    "            return\n",
    "\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "\n",
    "class RandomForest_Pro():\n",
    "    def __init__(self,\n",
    "                 n_estimators,\n",
    "                 max_features,\n",
    "                 boostrap=True,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.weak_clf = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, seeds):\n",
    "        for i in range(self.n_estimators):\n",
    "            # bootstrapping\n",
    "            if self.boostrap:\n",
    "                np.random.seed(seeds[i])\n",
    "                sample_idx = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            else:\n",
    "                sample_idx = np.arange(len(X))\n",
    "\n",
    "            clf = DecisionTree_Pro(criterion=self.criterion, max_depth=self.max_depth)\n",
    "            clf.fit(X[sample_idx], Y[sample_idx], max_features=self.max_features, init_seed=seeds[i])\n",
    "\n",
    "            y_pred =clf.predict(X)\n",
    "\n",
    "            if 0.75 < accuracy_score(Y, y_pred) < 0.85:\n",
    "                # print(accuracy_score(Y, y_pred))\n",
    "                self.weak_clf.append(clf)\n",
    "                # print(clf.feature_importance())\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.zeros(len(X_test), dtype='int64')\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred += self.weak_clf[i].predict(X_test)\n",
    "\n",
    "        mask = (y_pred > (self.n_estimators / 2))\n",
    "        y_pred[mask] = 1\n",
    "        y_pred[(mask == False)] = 0\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 15\n",
    "max_depth = 2\n",
    "max_features = np.sqrt(X_train.shape[1]).astype('uint32')\n",
    "\n",
    "while True:\n",
    "    seeds = np.random.choice(100000, size=n, replace=True)\n",
    "    print(seeds)\n",
    "    clf = RandomForest_Pro(n_estimators=n, max_features=max_features, boostrap=True, max_depth=max_depth)\n",
    "    clf.fit(X_train,y_train, seeds)\n",
    "    y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "    score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "    print(f'n = {n}, Train score={train_score}, Test score={score}')\n",
    "    \n",
    "    if score>0.85:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 15\n",
    "max_depth = 2\n",
    "max_features = np.sqrt(X_train.shape[1]).astype('uint32')\n",
    "\n",
    "for yeah in range(10):\n",
    "    seeds = np.array([40883, 66616, 93957, 77118, 24161, \n",
    "                      75479, 22292, 85542, 26905, 54760, \n",
    "                      51806, 58809, 84103, 97031, 9055])\n",
    "    # print(seeds)\n",
    "\n",
    "    clf = RandomForest_Pro(n_estimators=n, max_features=max_features, boostrap=True, max_depth=max_depth)\n",
    "    clf.fit(X_train, y_train, seeds)\n",
    "    y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "    score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "    print(f'n = {n}, Train score={train_score}, Test score={score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "max_depth = 2\n",
    "max_features = np.sqrt(X_train.shape[1]).astype('uint32')\n",
    "\n",
    "clf = RandomForest_Pro(n_estimators=n, max_features=max_features, boostrap=False, max_depth=max_depth)\n",
    "clf.fit(X_train, y_train, seeds)\n",
    "y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "print(f'n = {n}, Train score={train_score}, Test score={score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_depth=2\n",
    "max_features = np.sqrt(X_train.shape[1]).astype('uint32')\n",
    "# max_features = X_train.shape[1]\n",
    "for n in range(1, 32):\n",
    "    clf = RandomForest_Pro(n_estimators=n, max_features=max_features, boostrap=False, max_depth=max_depth)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred, y_pred_train = clf.predict(X_test), clf.predict(X_train)\n",
    "    score, train_score = accuracy_score(y_test, y_pred), accuracy_score(y_train, y_pred_train)\n",
    "    print(f'n = {n}, Train score={train_score}, Test score={score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
