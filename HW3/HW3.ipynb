{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7fa6f2-0cb0-438e-946c-39c4652e6188",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1844545-0edc-48f3-b449-7d8ecd7efc7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data\n",
    "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
    "See follow links for more information\n",
    "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de07a6f4-e9c5-4e22-82e7-84ef010703b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c22cbf5-f0a1-4674-8065-b5e50ac81c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "\n",
    "# drop some unwanted rows\n",
    "train_df = train_df.drop([248, 250, 251])  # cp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee830776-6656-4db7-a819-4aa3d6fc412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>fixed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>reversible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "136   54    1   2       192   283    0        2      195      0      0.0   \n",
       "232   58    0   4       170   225    1        2      146      1      2.8   \n",
       "233   56    1   2       130   221    0        2      163      0      0.0   \n",
       "184   46    1   4       120   249    0        2      144      0      0.8   \n",
       "84    55    0   2       135   250    0        2      161      0      1.4   \n",
       "\n",
       "     slope  ca        thal  target  \n",
       "136      1   1  reversible       0  \n",
       "232      2   2       fixed       1  \n",
       "233      1   0  reversible       0  \n",
       "184      1   0  reversible       0  \n",
       "84       2   0      normal       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19302aae-0347-440e-a818-148a7cf66be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df.info()\n",
    "# test_df.info()\n",
    "train_df.columns\n",
    "# test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e71ed-8344-4b66-a013-74476eb3915b",
   "metadata": {},
   "source": [
    "### one-hot encoding\n",
    "- features: 'cp', 'restecg', 'thal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408170d7-1e11-4040-993d-e5ac1af2b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding_list = ['cp', 'restecg', 'thal', 'slope', 'ca']\n",
    "encoding_list = ['cp', 'restecg', 'thal']\n",
    "for feature in encoding_list:\n",
    "    train_df = pd.get_dummies(train_df, columns=[feature], prefix=[feature])\n",
    "    test_df = pd.get_dummies(test_df, columns=[feature], prefix=[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdaf0f49-eba2-410d-be3f-38a6ddea2e90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
      "       'slope', 'ca', 'target', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0',\n",
      "       'restecg_1', 'restecg_2', 'thal_fixed', 'thal_normal',\n",
      "       'thal_reversible'],\n",
      "      dtype='object')\n",
      "Index(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
      "       'slope', 'ca', 'target', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0',\n",
      "       'restecg_1', 'restecg_2', 'thal_fixed', 'thal_normal',\n",
      "       'thal_reversible'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print(train_df.head())\n",
    "# print(test_df.head())\n",
    "print(train_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9654cd-ac6d-4d84-a6aa-ccaff7a78d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
      "       'slope', 'ca', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0', 'restecg_1',\n",
      "       'restecg_2', 'thal_fixed', 'thal_normal', 'thal_reversible'],\n",
      "      dtype='object')\n",
      "Index(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
      "       'slope', 'ca', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0', 'restecg_1',\n",
      "       'restecg_2', 'thal_fixed', 'thal_normal', 'thal_reversible'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "features = list(train_df.columns)\n",
    "features.remove('target')\n",
    "X_train_df, y_train_df = train_df[features], train_df['target']\n",
    "X_test_df, y_test_df = test_df[features], test_df['target']\n",
    "print(X_train_df.columns)\n",
    "print(X_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fada5c9a-2bc6-4690-b726-79f106f5a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_df.to_numpy(), y_train_df.to_numpy()\n",
    "X_test, y_test = X_test_df.to_numpy(), y_test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa6dcd0-a0d1-4f11-87e3-f98e84f820b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 20) (198,)\n",
      "(100, 20) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b8beb-1baf-4c81-9a0b-c4c0fd650e27",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0565d51b-a630-4ce1-91e7-53bcaa11ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    if np.unique(sequence).shape[0] == 1:  # pure\n",
    "        return 0\n",
    "\n",
    "    hist = np.bincount(sequence) / len(sequence)\n",
    "    return 1 - np.sum(hist ** 2)\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    if np.unique(sequence).shape[0] == 1:\n",
    "        return 0\n",
    "\n",
    "    hist = np.bincount(sequence) / len(sequence)\n",
    "    hist = hist[hist != 0]  # eliminate zeros\n",
    "    return - np.sum(hist * log2(hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6b8f8d-99ed-49b9-bccc-885567a80425",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969654be-5715-4bde-99b1-d11d6e5b6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a7e236-a703-4f6b-806f-b12de0352367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e2e88-39d9-467b-adb1-4a2b3fb734cc",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c87ab2-d592-4ea7-b554-72b5ebf62f45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, X: np.ndarray,\n",
    "                 Y: np.ndarray,\n",
    "                 depth, criterion='gini',\n",
    "                 max_depth=None,\n",
    "                 sample_weight=None,\n",
    "                 max_features=None):\n",
    "        \"\"\"\n",
    "            X(np arr): features\n",
    "            Y(np arr): labels\n",
    "        \"\"\"\n",
    "        self.right, self.left = None, None\n",
    "        self.X, self.Y = X, Y\n",
    "        self.depth = depth\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.best_feature, self.best_split_value = None, None\n",
    "        self.cls = None\n",
    "        self.is_leaf = False\n",
    "        self.sample_weight = sample_weight\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def gini(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        hist = hist[hist != 0]  # eliminate zeros\n",
    "        return - np.sum(hist * log2(hist))\n",
    "\n",
    "    def weighted_gini(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (gini) \"\"\"\n",
    "        return (len(Y_r) * self.gini(Y_r) + len(Y_l) * self.gini(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def weighted_entropy(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (entropy) \"\"\"\n",
    "        return (len(Y_r) * self.entropy(Y_r) + len(Y_l) * self.entropy(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def sample_weight_gini(self, Y: np.ndarray, sample_weight: np.ndarray):\n",
    "        cls_list = np.unique(Y)\n",
    "\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if len(cls_list) == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        cls_count = np.zeros(len(cls_list))\n",
    "\n",
    "        for cls in cls_list:\n",
    "            cls_count[cls] = sample_weight[Y == cls].sum()\n",
    "\n",
    "        hist = cls_count / sample_weight.sum()\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def sw_gini(self, Y_r, Y_l, sw_r, sw_l):\n",
    "        \"\"\"  sample weighted  \"\"\"\n",
    "        def gini(Y, sample_weight): return self.sample_weight_gini(\n",
    "            Y, sample_weight)\n",
    "        len_r, len_l = sw_r.sum(), sw_l.sum()\n",
    "        len_tot = len_r + len_l\n",
    "        return (len_r * gini(Y_r, sw_r) + len_l * gini(Y_l, sw_l)) / len_tot\n",
    "\n",
    "    def sw_best_split(self):\n",
    "        \"\"\" (w/ sample weight) find the best split \"\"\"\n",
    "        X, Y, sample_weight = self.X.copy(), self.Y.copy(), self.sample_weight.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        for feature in range(len(X[0])):\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                mask = (X[:, feature] == 0)\n",
    "                inv_mask = (mask == False)\n",
    "                Y_l, sw_l = Y[mask], sample_weight[mask]\n",
    "                Y_r, sw_r = Y[inv_mask], sample_weight[inv_mask]\n",
    "\n",
    "                Gini = self.sw_gini(Y_r, Y_l, sw_r, sw_l)\n",
    "                # print('feature:', feature,  'gini:',Gini)  #FIXME\n",
    "                if Gini < min_gini:\n",
    "                    min_gini = Gini\n",
    "                    best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y, sample_weight = X[idx], Y[idx], sample_weight[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    mask = (X[:, feature] <= threshold)\n",
    "                    inv_mask = (mask == False)\n",
    "                    Y_l, sw_l = Y[mask], sample_weight[mask]\n",
    "                    Y_r, sw_r = Y[inv_mask], sample_weight[inv_mask]\n",
    "\n",
    "                    Gini = self.sw_gini(Y_r, Y_l, sw_r, sw_l)\n",
    "                    # print('feature:', feature,  'gini:',Gini)  #FIXME\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def sw_split(self):\n",
    "        \"\"\" (w/ sample weight) split this node recursively \"\"\"\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.sw_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l, sw_l = self.X[mask_l,\n",
    "                                :], self.Y[mask_l], self.sample_weight[mask_l]\n",
    "        X_r, Y_r, sw_r = self.X[mask_r,\n",
    "                                :], self.Y[mask_r], self.sample_weight[mask_r]\n",
    "\n",
    "        lnode = Node(X_l, Y_l, self.depth + 1,\n",
    "                     self.criterion, self.max_depth, sw_l)\n",
    "        rnode = Node(X_r, Y_r, self.depth + 1,\n",
    "                     self.criterion, self.max_depth, sw_r)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "\n",
    "        rnode.sw_split()\n",
    "        lnode.sw_split()\n",
    "\n",
    "    def rand_best_split(self):\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        # randomly sample features\n",
    "        feature_idx = np.random.choice(\n",
    "            len(X[0]), size=self.max_features, replace=False)\n",
    "\n",
    "        for feature in feature_idx:\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def best_split(self):\n",
    "        \"\"\" find the best split \"\"\"\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        for feature in range(len(X[0])):\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" split this node recursively \"\"\"\n",
    "\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "        # FIXME\n",
    "        # print('depth:', self.depth)\n",
    "        # print('cls 0:', cls_count[0])\n",
    "        # print('cls 1:', cls_count[1])\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "\n",
    "            # FIXME\n",
    "            # print('predicted class:', self.cls)\n",
    "            # print()\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.best_split() if self.max_features is None \\\n",
    "            else self.rand_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # FIXME\n",
    "        # print('best_feature: ', best_feature, 'best_split_value: ', best_split_value)\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l = self.X[mask_l, :], self.Y[mask_l]\n",
    "        X_r, Y_r = self.X[mask_r, :], self.Y[mask_r]\n",
    "\n",
    "        lnode = Node(X_l, Y_l, self.depth + 1, self.criterion, self.max_depth)\n",
    "        rnode = Node(X_r, Y_r, self.depth + 1, self.criterion, self.max_depth)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "        # print()\n",
    "\n",
    "        rnode.split()\n",
    "        lnode.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a16c73-08b2-4bc1-9fba-f7db175f6fbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        if criterion not in ['gini', 'entropy']:\n",
    "            raise Exception(\"supported criterion: 'gini' or 'entropy'\")\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, sample_weight=None, max_features=None):\n",
    "        \"\"\" Build a decision tree classifier from the training set (X, y).\"\"\"\n",
    "        self.X, self.Y = X.copy(), Y.copy()\n",
    "        self.root = Node(self.X,\n",
    "                         self.Y,\n",
    "                         depth=0,\n",
    "                         criterion=self.criterion,\n",
    "                         max_depth=self.max_depth,\n",
    "                         sample_weight=sample_weight,\n",
    "                         max_features=max_features)\n",
    "\n",
    "        if sample_weight is None:\n",
    "            self.root.split()\n",
    "        else:\n",
    "            self.root.sw_split()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict class value for X. \"\"\"\n",
    "        y_pred = []\n",
    "\n",
    "        for row in X_test:\n",
    "            cur_node = self.root\n",
    "\n",
    "            while not cur_node.is_leaf:\n",
    "                bf = cur_node.best_feature\n",
    "                cur_node = cur_node.left if row[bf] <= cur_node.best_split_value else cur_node.right\n",
    "\n",
    "            y_pred.append(cur_node.cls)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def feature_importance(self) -> np.ndarray:\n",
    "        \"\"\" count occurrence of each best_feature \"\"\"\n",
    "        cur_node = self.root\n",
    "\n",
    "        self.feature_count = np.zeros(len(self.X[0]), dtype='uint8')\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "        return self.feature_count\n",
    "\n",
    "    def count_feature_util(self, cur_node: Node):\n",
    "        if cur_node.is_leaf:\n",
    "            return\n",
    "\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db4b37-9adb-4175-b3c8-9a7f32888d47",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce15fa6-e6d3-41a2-940e-451e6a6c41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.fit(X_train, y_train)\n",
    "y_pred = clf_depth3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7798d859-580b-4768-813f-0f71aa12504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 3\n",
      "Test-set accuarcy score:  0.79\n"
     ]
    }
   ],
   "source": [
    "print('Depth = 3')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f898ff-026a-40da-967f-ce7aa719596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "clf_depth10.fit(X_train, y_train)\n",
    "y_pred = clf_depth10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8f7801b-9bf0-4bb1-a2c6-612011b285a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 10\n",
      "Test-set accuarcy score:  0.76\n"
     ]
    }
   ],
   "source": [
    "print('Depth = 10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ab193-cc53-4581-83b3-3542633351a1",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8d0f176-856c-4310-92ad-8e637f652a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "y_pred = clf_gini.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16e8ab29-cee2-4f78-9ad3-c1592c26421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini\n",
      "Test-set accuarcy score:  0.79\n"
     ]
    }
   ],
   "source": [
    "print('Gini')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5baf3e4d-9448-4ec5-b525-cae0d477fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.fit(X_train, y_train)\n",
    "y_pred = clf_entropy.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a117957-9259-42c1-a4e2-7bcc7d9ca682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entopry\n",
      "Test-set accuarcy score:  0.79\n"
     ]
    }
   ],
   "source": [
    "print('Entopry')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e8da3-1879-4d33-ba4c-8457e1e1fd7a",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65a20c-51f5-46c6-9c3b-ed660cc8cc18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13b12ac5-f6f1-4e32-818a-832ba01a6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
    "        'slope', 'ca', 'cp','restecg','thal'])\n",
    "\n",
    "# ['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
    "#        'slope', 'ca', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0',\n",
    "#        'restecg_1', 'restecg_2', 'thal_fixed', 'thal_normal',\n",
    "#        'thal_reversible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50ed8e3a-3023-4dde-b8ea-b446dba6a9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 1 4 0 5 0 4 0 1 0 0 0 1 0 0 0 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "tmp = clf_depth10.feature_importance()\n",
    "hist = np.zeros(13, dtype='uint8')\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc75771a-53c6-404b-967d-23cc219cd532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 1 4 0 5 0 4 0 1 1 0 3]\n"
     ]
    }
   ],
   "source": [
    "hist[0:10] = tmp[0:10]\n",
    "hist[-3:] = [tmp[10:10+4].sum(), tmp[14:14+3].sum(), tmp[17:17+3].sum()]\n",
    "print(hist)\n",
    "mask_sort = hist.argsort()\n",
    "# print(mask_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c1748f4-2e62-4321-a73d-8d0f2e195fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title('Feature Importance', {'fontsize': 20})\n",
    "ax.barh(features[mask_sort], hist[mask_sort])\n",
    "\n",
    "for i in range(10):\n",
    "    ax.axvline(x=i, linewidth=1, color='0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2e780-b5c6-4bd5-b11a-eab60da87e60",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85252239-8345-4703-9bd9-ace31a1bf983",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.weak_clf = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        self.alphas = []\n",
    "        n = self.n_estimators\n",
    "        max_depth = 1\n",
    "\n",
    "        self.Y = Y.copy()\n",
    "        # change label: 0 -> -1\n",
    "        self.Y[Y == 0] = -1\n",
    "\n",
    "        data_w = np.ones(len(Y)) / len(Y)\n",
    "\n",
    "        # train n weak classifiers\n",
    "        for i in range(n):\n",
    "            # FIXME:\n",
    "            # print(i, \"th weak clf\")\n",
    "\n",
    "            # Fit weak classifier\n",
    "            clf = DecisionTree(criterion='gini', max_depth=max_depth)\n",
    "            clf.fit(X, Y, sample_weight=data_w)\n",
    "            y_pred = clf.predict(X)\n",
    "            y_pred[y_pred == 0] = -1\n",
    "\n",
    "            # error of the clf prediction\n",
    "            error = data_w[self.Y != y_pred].sum()\n",
    "\n",
    "            # handle boundaries\n",
    "            if error >= 1.0:\n",
    "                break\n",
    "            elif error <= 0.0:\n",
    "                alpha = 200.0\n",
    "            else:\n",
    "                alpha = np.log((1 - error) / error)\n",
    "\n",
    "            self.weak_clf.append(clf)\n",
    "            # alpha: weight of classifier\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            #  Update data weights\n",
    "            data_w = data_w * np.exp(- self.alphas[i] * self.Y * y_pred)\n",
    "            data_w = data_w / data_w.sum()\n",
    "\n",
    "            # FIXME\n",
    "            # print(\"error: \", error)\n",
    "            # print(\"alpha: \", alpha)\n",
    "            # print()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        prediction = np.zeros(len(X_test))\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred = self.weak_clf[i].predict(X_test)\n",
    "            y_pred[y_pred == 0] = -1\n",
    "            prediction += self.alphas[i] * y_pred\n",
    "\n",
    "        prediction = np.sign(prediction)\n",
    "        prediction[prediction < 0] = 0\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0355b8-7978-4159-84e3-d593cc67de38",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "874cc412-9c6a-4d70-b3b9-2db4fa22f8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_Ada_10 = AdaBoost(n_estimators=10)\n",
    "clf_Ada_10.fit(X_train, y_train)\n",
    "y_pred = clf_Ada_10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138f7490-0d58-4381-aa66-133ed6de0a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10\n",
      "Test-set accuarcy score:  0.81\n"
     ]
    }
   ],
   "source": [
    "print('n_estimators=10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db876b3f-2b62-4829-acb5-57c1bd4ff6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_Ada_100 = AdaBoost(n_estimators=100)\n",
    "clf_Ada_100.fit(X_train, y_train)\n",
    "y_pred = clf_Ada_100.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e526727-e8b2-43fc-bc06-bb031629d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100\n",
      "Test-set accuarcy score:  0.81\n"
     ]
    }
   ],
   "source": [
    "print('n_estimators=100')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a21815-0978-43c2-b3a1-078b64013cb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6d60315-e96a-49a7-b241-d86e78d33237",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self,\n",
    "                 n_estimators,\n",
    "                 max_features,\n",
    "                 boostrap=True,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.weak_clf = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        for i in range(self.n_estimators):\n",
    "            # bootstrapping\n",
    "            if self.boostrap:\n",
    "                sample_idx = np.random.choice(\n",
    "                    len(X), size=len(X), replace=True)\n",
    "            else:\n",
    "                sample_idx = np.arange(len(X))\n",
    "\n",
    "            clf = DecisionTree(criterion=self.criterion,\n",
    "                               max_depth=self.max_depth)\n",
    "            clf.fit(X[sample_idx], Y[sample_idx],\n",
    "                    max_features=self.max_features)\n",
    "            self.weak_clf.append(clf)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.zeros(len(X_test), dtype='int64')\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred += self.weak_clf[i].predict(X_test)\n",
    "\n",
    "        mask = (y_pred > (self.n_estimators / 2))\n",
    "        y_pred[mask] = 1\n",
    "        y_pred[(mask == False)] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44027e-65af-477d-befd-d757f7aba6db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d551371-1c0a-4933-8fa8-6bb1f55a2554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10\n",
      "Test-set accuarcy score:  0.77\n"
     ]
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(\n",
    "    X_train.shape[1]).astype('uint32'), max_depth=None)\n",
    "clf_10tree.fit(X_train, y_train)\n",
    "y_pred = clf_10tree.predict(X_test)\n",
    "\n",
    "print('n_estimators=10')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1129298-73aa-4feb-9de0-c69d051082bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=100\n",
      "Test-set accuarcy score:  0.74\n"
     ]
    }
   ],
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(\n",
    "    X_train.shape[1]).astype('uint32'), max_depth=None)\n",
    "clf_100tree.fit(X_train, y_train)\n",
    "y_pred = clf_100tree.predict(X_test)\n",
    "\n",
    "print('n_estimators=100')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00115ef-6c14-411e-9513-b2fe320bb700",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1fb4f11-9aef-4e77-a1cb-8aa7423e1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=sqrt(n_features)\n",
      "Test-set accuarcy score:  0.74\n"
     ]
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(\n",
    "    X_train.shape[1]).astype('uint32'), max_depth=None)\n",
    "clf_random_features.fit(X_train, y_train)\n",
    "y_pred = clf_random_features.predict(X_test)\n",
    "\n",
    "print('max_features=sqrt(n_features)')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "732c68f9-8de8-4e4f-bb5a-bfe6c5b5d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=n_features\n",
      "Test-set accuarcy score:  0.83\n"
     ]
    }
   ],
   "source": [
    "clf_all_features = RandomForest(\n",
    "    n_estimators=10, max_features=X_train.shape[1], max_depth=None)\n",
    "clf_all_features.fit(X_train, y_train)\n",
    "y_pred = clf_all_features.predict(X_test)\n",
    "\n",
    "print('max_features=n_features')\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eea873-29c6-404d-92fc-8e82e16a7afc",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a99768-d984-4a46-86db-26c5c57cb0ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 6.\n",
    "Try you best to get highest test accuracy score by \n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7fb7d84-1c49-410f-9fd2-789821138b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node_Pro():\n",
    "    def __init__(self,\n",
    "                 X: np.ndarray,\n",
    "                 Y: np.ndarray,\n",
    "                 depth,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None,\n",
    "                 max_features=None,\n",
    "                 init_seed=None):\n",
    "        \"\"\"\n",
    "            X(np arr): features\n",
    "            Y(np arr): labels\n",
    "        \"\"\"\n",
    "        self.right, self.left = None, None\n",
    "        self.X, self.Y = X, Y\n",
    "        self.depth = depth\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.best_feature, self.best_split_value = None, None\n",
    "        self.cls = None\n",
    "        self.is_leaf = False\n",
    "        self.max_features = max_features\n",
    "        self.init_seed = init_seed\n",
    "\n",
    "    def gini(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:  # pure\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        return 1 - np.sum(hist ** 2)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        if len(Y) == 0:\n",
    "            return 0\n",
    "        if np.unique(Y).shape[0] == 1:\n",
    "            return 0\n",
    "\n",
    "        hist = np.bincount(Y) / len(Y)\n",
    "        hist = hist[hist != 0]  # eliminate zeros\n",
    "        return - np.sum(hist * log2(hist))\n",
    "\n",
    "    def weighted_gini(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (gini) \"\"\"\n",
    "        return (len(Y_r) * self.gini(Y_r) + len(Y_l) * self.gini(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def weighted_entropy(self, Y_r, Y_l):\n",
    "        \"\"\" weighted sum (entropy) \"\"\"\n",
    "        return (len(Y_r) * self.entropy(Y_r) + len(Y_l) * self.entropy(Y_l)) / (len(Y_r) + len(Y_l))\n",
    "\n",
    "    def rand_best_split(self):\n",
    "        ''' \n",
    "            sample a subset of features\n",
    "            used by Random forest \n",
    "        '''\n",
    "        X, Y = self.X.copy(), self.Y.copy()\n",
    "\n",
    "        min_gini = 0.5\n",
    "        min_entropy = 1.0\n",
    "\n",
    "        best_feature, best_split_value = None, None\n",
    "\n",
    "        # randomly sample features\n",
    "        np.random.seed(self.init_seed)\n",
    "        feature_idx = np.random.choice(\n",
    "            len(X[0]), size=self.max_features, replace=False)\n",
    "\n",
    "        for feature in feature_idx:\n",
    "            feature_val = np.unique(X[:, feature])\n",
    "\n",
    "            # skip this feature: no information\n",
    "            if len(feature_val) == 1:\n",
    "                continue\n",
    "\n",
    "            # TODO: categorical features (after one-hot-encoding)\n",
    "            #   value = 0 or 1\n",
    "            if np.array_equal(feature_val, [0.0, 1.0]):\n",
    "                # split\n",
    "                Y_l = Y[X[:, feature] == 0]\n",
    "                Y_r = Y[X[:, feature] == 1]\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                    if Gini < min_gini:\n",
    "                        min_gini = Gini\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "                elif self.criterion == 'entropy':\n",
    "                    Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                    if Entropy < min_entropy:\n",
    "                        min_entropy = Entropy\n",
    "                        best_feature, best_split_value = feature, 0\n",
    "\n",
    "            # TODO: continuous features\n",
    "            else:\n",
    "                # sort by current feature\n",
    "                idx = np.argsort(X[:, feature])\n",
    "                X, Y = X[idx], Y[idx]\n",
    "\n",
    "                # test different split point\n",
    "                for threshold in feature_val[: -1]:\n",
    "                    # split\n",
    "                    Y_l = Y[X[:, feature] <= threshold]\n",
    "                    Y_r = Y[X[:, feature] > threshold]\n",
    "\n",
    "                    if self.criterion == 'gini':\n",
    "                        Gini = self.weighted_gini(Y_r, Y_l)\n",
    "                        if Gini < min_gini:\n",
    "                            min_gini = Gini\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "                    elif self.criterion == 'entropy':\n",
    "                        Entropy = self.weighted_entropy(Y_r, Y_l)\n",
    "                        if Entropy < min_entropy:\n",
    "                            min_entropy = Entropy\n",
    "                            best_feature, best_split_value = feature, threshold\n",
    "\n",
    "        return best_feature, best_split_value\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" split this node recursively \"\"\"\n",
    "\n",
    "        cls_count = np.bincount(self.Y, minlength=2)\n",
    "        # FIXME\n",
    "        # print('depth:', self.depth)\n",
    "        # print('cls 0:', cls_count[0])\n",
    "        # print('cls 1:', cls_count[1])\n",
    "\n",
    "        # make leaf node: reach depth limit or pure node\n",
    "        if (self.max_depth is not None and self.depth == self.max_depth) or cls_count[0] == 0 or \\\n",
    "                cls_count[1] == 0:\n",
    "            self.cls = np.argmax(cls_count)  # majority class\n",
    "            self.is_leaf = True\n",
    "\n",
    "            # FIXME\n",
    "            # print('predicted class:', self.cls)\n",
    "            # print()\n",
    "            return\n",
    "\n",
    "        # find best split\n",
    "        best_feature, best_split_value = self.rand_best_split()\n",
    "        self.best_feature, self.best_split_value = best_feature, best_split_value\n",
    "\n",
    "        # FIXME\n",
    "        # print('best_feature: ', best_feature, 'best_split_value: ', best_split_value)\n",
    "\n",
    "        # split data\n",
    "        # ex: 0, 1; <=50 , >50;\n",
    "        mask_l = (self.X[:, best_feature] <= best_split_value)\n",
    "        mask_r = (mask_l == False)\n",
    "        X_l, Y_l = self.X[mask_l, :], self.Y[mask_l]\n",
    "        X_r, Y_r = self.X[mask_r, :], self.Y[mask_r]\n",
    "\n",
    "        lnode = Node_Pro(X_l, Y_l, self.depth + 1, self.criterion,\n",
    "                         self.max_depth, self.max_features, 2*self.init_seed)\n",
    "        rnode = Node_Pro(X_r, Y_r, self.depth + 1, self.criterion,\n",
    "                         self.max_depth, self.max_features, 2*self.init_seed+1)\n",
    "\n",
    "        # recursively split children\n",
    "        self.right, self.left = rnode, lnode\n",
    "        # print()\n",
    "\n",
    "        rnode.split()\n",
    "        lnode.split()\n",
    "\n",
    "\n",
    "class DecisionTree_Pro():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        if criterion not in ['gini', 'entropy']:\n",
    "            raise Exception(\"supported criterion: 'gini' or 'entropy'\")\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, max_features=None, init_seed=None):\n",
    "        \"\"\" Build a decision tree classifier from the training set (X, y).\"\"\"\n",
    "        self.X, self.Y = X.copy(), Y.copy()\n",
    "        self.root = Node_Pro(self.X,\n",
    "                             self.Y,\n",
    "                             depth=0,\n",
    "                             criterion=self.criterion,\n",
    "                             max_depth=self.max_depth,\n",
    "                             max_features=max_features, init_seed=init_seed)\n",
    "\n",
    "        self.root.split()\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict class value for X. \"\"\"\n",
    "        y_pred = []\n",
    "\n",
    "        for row in X_test:\n",
    "            cur_node = self.root\n",
    "\n",
    "            while not cur_node.is_leaf:\n",
    "                bf = cur_node.best_feature\n",
    "                cur_node = cur_node.left if row[bf] <= cur_node.best_split_value else cur_node.right\n",
    "\n",
    "            y_pred.append(cur_node.cls)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def feature_importance(self) -> np.ndarray:\n",
    "        \"\"\" count occurrence of each best_feature \"\"\"\n",
    "        cur_node = self.root\n",
    "\n",
    "        self.feature_count = np.zeros(len(self.X[0]), dtype='uint8')\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "        return self.feature_count\n",
    "\n",
    "    def count_feature_util(self, cur_node):\n",
    "        if cur_node.is_leaf:\n",
    "            return\n",
    "\n",
    "        self.feature_count[cur_node.best_feature] += 1\n",
    "        self.count_feature_util(cur_node.right)\n",
    "        self.count_feature_util(cur_node.left)\n",
    "\n",
    "\n",
    "class RandomForest_Pro():\n",
    "    def __init__(self,\n",
    "                 n_estimators,\n",
    "                 max_features,\n",
    "                 boostrap=True,\n",
    "                 criterion='gini',\n",
    "                 max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.weak_clf = []\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, seeds):\n",
    "        for i in range(self.n_estimators):\n",
    "            # bootstrapping\n",
    "            if self.boostrap:\n",
    "                np.random.seed(seeds[i])\n",
    "                sample_idx = np.random.choice(\n",
    "                    len(X), size=len(X), replace=True)\n",
    "            else:\n",
    "                sample_idx = np.arange(len(X))\n",
    "\n",
    "            clf = DecisionTree_Pro(\n",
    "                criterion=self.criterion, max_depth=self.max_depth)\n",
    "            clf.fit(X[sample_idx], Y[sample_idx],\n",
    "                    max_features=self.max_features, init_seed=seeds[i])\n",
    "\n",
    "            y_pred = clf.predict(X)\n",
    "\n",
    "            if 0.75 < accuracy_score(Y, y_pred) < 0.85:\n",
    "                # print(accuracy_score(Y, y_pred))\n",
    "                self.weak_clf.append(clf)\n",
    "                # print(clf.feature_importance())\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.zeros(len(X_test), dtype='int64')\n",
    "        for i in range(len(self.weak_clf)):\n",
    "            y_pred += self.weak_clf[i].predict(X_test)\n",
    "\n",
    "        mask = (y_pred > (self.n_estimators / 2))\n",
    "        y_pred[mask] = 1\n",
    "        y_pred[(mask == False)] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a3ad4cf-9479-4cba-807f-9f6991ab8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score=0.86\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "max_depth = 2\n",
    "max_features = np.sqrt(X_train.shape[1]).astype('uint32')\n",
    "\n",
    "seeds = np.array([40883, 66616, 93957, 77118, 24161,\n",
    "                  75479, 22292, 85542, 26905, 54760,\n",
    "                  51806, 58809, 84103, 97031, 9055])\n",
    "# print(seeds)\n",
    "\n",
    "clf = RandomForest_Pro(\n",
    "    n_estimators=n, max_features=max_features, boostrap=True, max_depth=max_depth)\n",
    "clf.fit(X_train, y_train, seeds)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'Test score={accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8c1b7-26ec-43a4-81a7-c519f25dd3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cae6f28a7b68e75a1329b9a2aaa47f0243b479a8d17df67ce3a643e470e86c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
